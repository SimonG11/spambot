{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ba13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cde2aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the .names file\n",
    "spambase_names_path = 'spambase.names'\n",
    "\n",
    "# Function to read and extract feature names from the .names file\n",
    "def extract_feature_names(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Skip the first 33 lines and start reading from line 34\n",
    "        lines = file.readlines()[33:]\n",
    "    # Extract feature names from the file content\n",
    "    feature_names = []\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            name = line.split(':')[0]\n",
    "            feature_names.append(name)\n",
    "    return feature_names\n",
    "\n",
    "# Adding names\n",
    "feature_names = extract_feature_names(spambase_names_path)\n",
    "feature_names.append('is_spam')  # Adding the class label since it is not defined as the last column name\n",
    "\n",
    "# Load the dataset with names\n",
    "spambase_data_path = 'spambase.data'\n",
    "data = pd.read_csv(spambase_data_path, names=feature_names)\n",
    "\n",
    "# Separate X and y\n",
    "X = data.drop('is_spam', axis=1)\n",
    "y = data['is_spam']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd117b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b30555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_features='sqrt'),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f180a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cb130d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics to compute\n",
    "scoring = ['accuracy', 'f1']\n",
    "\n",
    "# Initialize a dictionary to store all results\n",
    "results = {'Accuracy': {name: [] for name in classifiers.keys()},\n",
    "           'F-Measure': {name: [] for name in classifiers.keys()},\n",
    "           'Training Time': {name: [] for name in classifiers.keys()}}\n",
    "\n",
    "# Perform cross-validation for each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    cv_results = cross_validate(clf, X_train, y_train, cv=skf, scoring=scoring, return_train_score=False, n_jobs=-1)\n",
    "\n",
    "    # Append the results for each fold to the results dictionary\n",
    "    results['Accuracy'][name] = cv_results['test_accuracy']\n",
    "    results['F-Measure'][name] = cv_results['test_f1']\n",
    "    results['Training Time'][name] = cv_results['fit_time']\n",
    "\n",
    "# Convert results for each metric into a DataFrame\n",
    "for metric in results:\n",
    "    results[metric] = pd.DataFrame(results[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e698227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each classifier in each metric\n",
    "final_results = {}\n",
    "for metric, df in results.items():\n",
    "    means = df.mean(axis=0).rename('Mean')\n",
    "    stds = df.std(axis=0).rename('Std')\n",
    "    final_results[metric] = pd.concat([means, stds], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7da510cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without Ranks:\n",
      "-------------------------------------------------------\n",
      "    Random Forest  Naive Bayes  Gradient Boosting\n",
      "-------------------------------------------------------\n",
      "0   0.975543  0.880435  0.967391\n",
      "1   0.975543  0.804348  0.970109\n",
      "2   0.959239  0.796196  0.951087\n",
      "3   0.967391  0.823370  0.956522\n",
      "4   0.964674  0.807065  0.951087\n",
      "5   0.940217  0.815217  0.940217\n",
      "6   0.932065  0.807065  0.921196\n",
      "7   0.948370  0.826087  0.921196\n",
      "8   0.951087  0.823370  0.942935\n",
      "9   0.945652  0.820652  0.932065\n",
      "-------------------------------------------------------\n",
      "mean 0.955978  0.820380  0.945380\n",
      "std 0.014873  0.023286  0.017208\n",
      "-------------------------------------------------------\n",
      "\n",
      "F-Measure without Ranks:\n",
      "-------------------------------------------------------\n",
      "    Random Forest  Naive Bayes  Gradient Boosting\n",
      "-------------------------------------------------------\n",
      "0   0.968858  0.865031  0.958621\n",
      "1   0.968641  0.796610  0.961404\n",
      "2   0.947735  0.787535  0.937063\n",
      "3   0.958333  0.811594  0.944056\n",
      "4   0.955326  0.791789  0.937063\n",
      "5   0.924138  0.806818  0.924658\n",
      "6   0.912892  0.793003  0.899654\n",
      "7   0.931900  0.809524  0.894545\n",
      "8   0.936170  0.807122  0.925795\n",
      "9   0.929078  0.805882  0.911661\n",
      "-------------------------------------------------------\n",
      "mean 0.943307  0.807491  0.929452\n",
      "std 0.019324  0.021879  0.022755\n",
      "-------------------------------------------------------\n",
      "\n",
      "Training Time without Ranks:\n",
      "-------------------------------------------------------\n",
      "    Random Forest  Naive Bayes  Gradient Boosting\n",
      "-------------------------------------------------------\n",
      "0   0.412612  0.007975  0.965760\n",
      "1   0.426564  0.007975  1.003632\n",
      "2   0.440517  0.007975  0.991672\n",
      "3   0.437528  0.007975  0.944830\n",
      "4   0.413670  0.007975  0.957787\n",
      "5   0.427568  0.007972  0.972737\n",
      "6   0.392682  0.006977  1.004776\n",
      "7   0.393679  0.005980  0.963766\n",
      "8   0.436538  0.003933  0.952803\n",
      "9   0.395671  0.005927  0.936857\n",
      "-------------------------------------------------------\n",
      "mean 0.417703  0.007066  0.969462\n",
      "std 0.018782  0.001384  0.023684\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, to print the results in the desired format\n",
    "for metric in results:\n",
    "    print(f\"{metric} without Ranks:\")\n",
    "    print('-'*55)\n",
    "    \n",
    "    # Print the classifier names\n",
    "    classifier_names = results[metric].columns\n",
    "    print('   ', '  '.join(classifier_names))\n",
    "    print('-'*55)\n",
    "    \n",
    "    # Print each row of data (for each fold)\n",
    "    for index in range(len(results[metric])):\n",
    "        row_data = [results[metric][classifier].iloc[index] for classifier in classifier_names]\n",
    "        print(f\"{index:<3}\", '  '.join(f\"{val:.6f}\" for val in row_data))\n",
    "    \n",
    "    print('-'*55)\n",
    "\n",
    "    # Calculate and print mean and std for each classifier\n",
    "    means = results[metric].mean()\n",
    "    stds = results[metric].std()\n",
    "\n",
    "    print(\"mean\", '  '.join(f\"{means[name]:.6f}\" for name in classifier_names))\n",
    "    print(\"std\", '  '.join(f\"{stds[name]:.6f}\" for name in classifier_names))\n",
    "    print('-'*55)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5eb9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the results DataFrame with ranks for each fold\n",
    "for metric, df in results.items():\n",
    "    ascending = True if metric == 'Training Time' else False\n",
    "    ranks = df.rank(axis=1, ascending=ascending)\n",
    "    \n",
    "    ranked_df = df.copy()\n",
    "    for col in ranked_df.columns:\n",
    "        ranked_df[col] = ranked_df[col].round(6).astype(str) + \" (\" + ranks[col].astype(int).astype(str) + \")\"\n",
    "    \n",
    "    results[metric] = ranked_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80570500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Ranks:\n",
      "-------------------------------------------------------\n",
      "   Random Forest  Naive Bayes  Gradient Boosting\n",
      "-------------------------------------------------------\n",
      "0 0.978261 (1)  0.880435 (3)  0.967391 (2)\n",
      "1 0.975543 (1)  0.804348 (3)  0.970109 (2)\n",
      "2 0.956522 (1)  0.796196 (3)  0.951087 (2)\n",
      "3 0.961957 (1)  0.82337 (3)  0.956522 (2)\n",
      "4 0.970109 (1)  0.807065 (3)  0.951087 (2)\n",
      "5 0.942935 (1)  0.815217 (3)  0.940217 (2)\n",
      "6 0.932065 (1)  0.807065 (3)  0.921196 (2)\n",
      "7 0.942935 (1)  0.826087 (3)  0.921196 (2)\n",
      "8 0.951087 (1)  0.82337 (3)  0.942935 (2)\n",
      "9 0.94837 (1)  0.820652 (3)  0.932065 (2)\n",
      "-------------------------------------------------------\n",
      "Average Rank 1.0  3.0  2.0\n",
      "\n",
      "F-Measure with Ranks:\n",
      "-------------------------------------------------------\n",
      "   Random Forest  Naive Bayes  Gradient Boosting\n",
      "-------------------------------------------------------\n",
      "0 0.972414 (1)  0.865031 (3)  0.958621 (2)\n",
      "1 0.968641 (1)  0.79661 (3)  0.961404 (2)\n",
      "2 0.944056 (1)  0.787535 (3)  0.937063 (2)\n",
      "3 0.951389 (1)  0.811594 (3)  0.944056 (2)\n",
      "4 0.962199 (1)  0.791789 (3)  0.937063 (2)\n",
      "5 0.927835 (1)  0.806818 (3)  0.924658 (2)\n",
      "6 0.912892 (1)  0.793003 (3)  0.899654 (2)\n",
      "7 0.924731 (1)  0.809524 (3)  0.894545 (2)\n",
      "8 0.93662 (1)  0.807122 (3)  0.926316 (2)\n",
      "9 0.932384 (1)  0.805882 (3)  0.911661 (2)\n",
      "-------------------------------------------------------\n",
      "Average Rank 1.0  3.0  2.0\n",
      "\n",
      "Training Time with Ranks:\n",
      "-------------------------------------------------------\n",
      "   Random Forest  Naive Bayes  Gradient Boosting\n",
      "-------------------------------------------------------\n",
      "0 0.411619 (2)  0.004983 (1)  0.899979 (3)\n",
      "1 0.440523 (2)  0.005981 (1)  0.916923 (3)\n",
      "2 0.41163 (2)  0.00503 (1)  0.890013 (3)\n",
      "3 0.454474 (2)  0.004984 (1)  0.905959 (3)\n",
      "4 0.403646 (2)  0.008972 (1)  0.972735 (3)\n",
      "5 0.422582 (2)  0.005982 (1)  0.935858 (3)\n",
      "6 0.416601 (2)  0.006978 (1)  0.953799 (3)\n",
      "7 0.442515 (2)  0.007973 (1)  0.973732 (3)\n",
      "8 0.417599 (2)  0.004983 (1)  0.934862 (3)\n",
      "9 0.444508 (2)  0.00598 (1)  0.929878 (3)\n",
      "-------------------------------------------------------\n",
      "Average Rank 2.0  1.0  3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your existing code for updating the results DataFrame with ranks...\n",
    "\n",
    "# Print the results with ranks and calculate average ranks\n",
    "for metric, df in results.items():\n",
    "    print(f\"{metric} with Ranks:\")\n",
    "    print('-'*55)\n",
    "    \n",
    "    # Print the classifier names\n",
    "    classifier_names = df.columns\n",
    "    print('  ', '  '.join(classifier_names))\n",
    "    print('-'*55)\n",
    "    \n",
    "    # Print each row of data with ranks\n",
    "    for index, row in df.iterrows():\n",
    "        formatted_row = [f\"{value}\" for value in row]\n",
    "        print(index, '  '.join(formatted_row))\n",
    "    \n",
    "    print('-'*55)\n",
    "\n",
    "    # Calculate and print average rank for each classifier\n",
    "    avg_ranks = df.applymap(lambda x: int(x.split('(')[1].replace(')', ''))).mean(axis=0)\n",
    "    print(\"Average Rank\", '  '.join(f\"{avg_ranks[name]:.1f}\" for name in classifier_names))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae1aa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman Statistic and Degrees of Freedom for Each Metric:\n",
      "Accuracy: Chi2 = 11.000000000000014, df = 2\n",
      "F-Measure: Chi2 = 13.299999999999983, df = 2\n",
      "Training Time: Chi2 = 20.0, df = 2\n"
     ]
    }
   ],
   "source": [
    "def friedman_statistic(avg_ranks, N, k):\n",
    "    # Calculation of Friedman statistic\n",
    "    sum_of_squares = np.sum(avg_ranks ** 2)\n",
    "    chi2 = (12 * N / (k * (k + 1))) * (sum_of_squares - (k * (k + 1) ** 2 / 4))\n",
    "    degrees_of_freedom = k - 1\n",
    "\n",
    "    return chi2, degrees_of_freedom\n",
    "\n",
    "# Usage of the function\n",
    "N = 10  # Number of datasets (folds)\n",
    "k = 3   # Number of algorithms\n",
    "\n",
    "avg_ranks_accuracy = np.array([1.1, 3, 1.7])  # Accuracy\n",
    "avg_ranks_fmeasure = np.array([1.2, 3.0, 1.7])  # F-Measure\n",
    "avg_ranks_training_time = np.array([2.0, 1.0, 3.0])  # Training Time\n",
    "\n",
    "chi2_accuracy, df_accuracy = friedman_statistic(avg_ranks_accuracy, N, k)\n",
    "chi2_fmeasure, df_fmeasure = friedman_statistic(avg_ranks_fmeasure, N, k)\n",
    "chi2_training_time, df_training_time = friedman_statistic(avg_ranks_training_time, N, k)\n",
    "\n",
    "print(\"Friedman Statistic and Degrees of Freedom for Each Metric:\")\n",
    "print(f\"Accuracy: Chi2 = {chi2_accuracy}, df = {df_accuracy}\")\n",
    "print(f\"F-Measure: Chi2 = {chi2_fmeasure}, df = {df_fmeasure}\")\n",
    "print(f\"Training Time: Chi2 = {chi2_training_time}, df = {df_training_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f171797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significance and Critical Differences for Each Metric:\n",
      "Accuracy: Significance = True, CD = 1.2199986885238854\n",
      "F-Measure: Significance = True, CD = 1.2199986885238854\n",
      "Training Time: Significance= True, CD = 1.2199986885238854\n"
     ]
    }
   ],
   "source": [
    "def is_significant(chi2_statistic):\n",
    "    critical_value = 7.8\n",
    "    return chi2_statistic > critical_value\n",
    "\n",
    "def nemenyi_critical_difference(N, k, alpha=0.05):\n",
    "    # Critical value q_alpha for the studentized range statistic (to be verified)\n",
    "    q_alpha = 2.728  # Example value\n",
    "    return q_alpha * np.sqrt((k * (k + 1)) / (6 * N))\n",
    "\n",
    "\n",
    "# Check significance for each metric and calculate critical differences if significant\n",
    "significance_accuracy = is_significant(chi2_accuracy)\n",
    "significance_fmeasure = is_significant(chi2_fmeasure)\n",
    "significance_training_time = is_significant(chi2_training_time)\n",
    "\n",
    "cd_accuracy = nemenyi_critical_difference(N, k) if significance_accuracy else None\n",
    "cd_fmeasure = nemenyi_critical_difference(N, k) if significance_fmeasure else None\n",
    "cd_training_time = nemenyi_critical_difference(N, k) if significance_training_time else None\n",
    "\n",
    "# Print results\n",
    "print(\"Significance and Critical Differences for Each Metric:\")\n",
    "print(f\"Accuracy: Significance = {significance_accuracy}, CD = {cd_accuracy}\")\n",
    "print(f\"F-Measure: Significance = {significance_fmeasure}, CD = {cd_fmeasure}\")\n",
    "print(f\"Training Time: Significance= {significance_training_time}, CD = {cd_training_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f6b3969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significance for accuracy :\n",
      "Random Forest and Naive Bayes are significantly differerent\n",
      "Naive Bayes and Gradient Boosting are significantly differerent\n",
      "Significance for fmeasure :\n",
      "Random Forest and Naive Bayes are significantly differerent\n",
      "Naive Bayes and Gradient Boosting are significantly differerent\n",
      "Significance for training_time :\n",
      "Naive Bayes and Gradient Boosting are significantly differerent\n"
     ]
    }
   ],
   "source": [
    "def significant_difference(rank_lst, cd):\n",
    "    result = []\n",
    "    for i in range(len(avg_ranks_cd[0])-1):\n",
    "        for j in range(len(rank_lst)):\n",
    "            if abs(rank_lst[i] - rank_lst[j]) > cd and {i, j} not in result:\n",
    "                result.append({i, j})\n",
    "    return result\n",
    "\n",
    "\n",
    "def show(lst):\n",
    "    for pair in lst:\n",
    "        print(f\"{cls[pair.pop()]} and {cls[pair.pop()]} are significantly differerent\")\n",
    "avg_ranks_with_cd = {\"accuracy\":[avg_ranks_accuracy, cd_accuracy], \"fmeasure\": [avg_ranks_fmeasure, cd_fmeasure], \"training_time\":[avg_ranks_training_time,cd_training_time]}\n",
    "cls = [\"Random Forest\", \"Naive Bayes\", \"Gradient Boosting\"]\n",
    "\n",
    "\n",
    "for measure, avg_ranks_cd in avg_ranks_with_cd.items():\n",
    "    print(\"Significance for\", measure, \":\")\n",
    "    if avg_ranks_cd[1]:\n",
    "        result = significant_difference(avg_ranks_cd[0], avg_ranks_cd[1])\n",
    "        show(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac51e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd2220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac31cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783ff5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cefa94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
